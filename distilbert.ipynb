{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyarrow 16.0.0\n",
      "Uninstalling pyarrow-16.0.0:\n",
      "  Successfully uninstalled pyarrow-16.0.0\n",
      "Found existing installation: datasets 2.19.0\n",
      "Uninstalling datasets-2.19.0:\n",
      "  Successfully uninstalled datasets-2.19.0\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-16.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/mamba/lib/python3.11/site-packages (from pyarrow) (1.26.4)\n",
      "Using cached pyarrow-16.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-16.0.0\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.11/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/mamba/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/mamba/lib/python3.11/site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/mamba/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/mamba/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/mamba/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/mamba/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/mamba/lib/python3.11/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /opt/mamba/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/mamba/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/mamba/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/mamba/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/mamba/lib/python3.11/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /opt/mamba/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/mamba/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/mamba/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/mamba/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/mamba/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/mamba/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/mamba/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/mamba/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "Installing collected packages: datasets\n",
      "Successfully installed datasets-2.19.0\n",
      "Requirement already satisfied: torch in /opt/mamba/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in /opt/mamba/lib/python3.11/site-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.11/site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/mamba/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/mamba/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/mamba/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/mamba/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/mamba/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/mamba/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/mamba/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/mamba/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/mamba/lib/python3.11/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/mamba/lib/python3.11/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/mamba/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/mamba/lib/python3.11/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/mamba/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/mamba/lib/python3.11/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/mamba/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/mamba/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/mamba/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/mamba/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers[torch] in /opt/mamba/lib/python3.11/site-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (2024.4.16)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (4.66.2)\n",
      "Requirement already satisfied: torch in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/mamba/lib/python3.11/site-packages (from transformers[torch]) (0.29.3)\n",
      "Requirement already satisfied: psutil in /opt/mamba/lib/python3.11/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/mamba/lib/python3.11/site-packages (from torch->transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/mamba/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers[torch]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.11/site-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.11/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/mamba/lib/python3.11/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: accelerate in /opt/mamba/lib/python3.11/site-packages (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/mamba/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.11/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /opt/mamba/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/mamba/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/mamba/lib/python3.11/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/mamba/lib/python3.11/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/mamba/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/mamba/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/mamba/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/mamba/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/mamba/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/mamba/lib/python3.11/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y pyarrow datasets\n",
    "!pip install --no-use-pep517 pyarrow\n",
    "!pip install datasets\n",
    "!pip install torch transformers\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import zipfile\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.cuda\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer les données d'un challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['civel/diffusion/hackathon-minarm-2024/AIVSAI/hack_train.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lister les fichiers d'un challenge\n",
    "fs.ls(\"civel/diffusion/hackathon-minarm-2024/AIVSAI/hack_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger les données dans le service\n",
    "PATH_IN = 'civel/diffusion/hackathon-minarm-2024/AIVSAI/hack_train.csv'\n",
    "fs.download(PATH_IN, 'data/hack_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hack_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Little disclaimer: this deals with US laws and...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Read: Mentally Retarded Downs. See, we've got ...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If any of you frequent rbadhistory, there is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I believe in a flat tax system, where everyone...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edit: Ok guy's, my views have been changed on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>cmv_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56814</th>\n",
       "      <td>We consider the recovery of a source term f (x...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56815</th>\n",
       "      <td>Self-supervised learning (SlfSL), aiming at le...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56816</th>\n",
       "      <td>Recurrent neural networks (RNNs) have achieved...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56817</th>\n",
       "      <td>Deep reinforcement learning (DRL) is a booming...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56818</th>\n",
       "      <td>As part of Smart Cities initiatives, national,...</td>\n",
       "      <td>1</td>\n",
       "      <td>sci_gen_human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56819 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label            src\n",
       "0      Little disclaimer: this deals with US laws and...      1      cmv_human\n",
       "1      Read: Mentally Retarded Downs. See, we've got ...      1      cmv_human\n",
       "2      If any of you frequent rbadhistory, there is a...      1      cmv_human\n",
       "3      I believe in a flat tax system, where everyone...      1      cmv_human\n",
       "4      Edit: Ok guy's, my views have been changed on ...      1      cmv_human\n",
       "...                                                  ...    ...            ...\n",
       "56814  We consider the recovery of a source term f (x...      1  sci_gen_human\n",
       "56815  Self-supervised learning (SlfSL), aiming at le...      1  sci_gen_human\n",
       "56816  Recurrent neural networks (RNNs) have achieved...      1  sci_gen_human\n",
       "56817  Deep reinforcement learning (DRL) is a booming...      1  sci_gen_human\n",
       "56818  As part of Smart Cities initiatives, national,...      1  sci_gen_human\n",
       "\n",
       "[56819 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9195656f9a459fb593ff1de0bdfaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['texts', 'labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create a Hugging Face Dataset from the data (if not already in one)\n",
    "data_dict = {\n",
    "    \"texts\": df['text'],\n",
    "    \"labels\": df['label']\n",
    "}\n",
    "hf_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Define the preprocessing function to tokenize the data\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    tokenized_inputs = tokenizer(examples['texts'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Prepare the dictionary correctly.\n",
    "    return {\n",
    "        'input_ids': tokenized_inputs['input_ids'], \n",
    "        'attention_mask': tokenized_inputs['attention_mask'], \n",
    "        'labels': examples['labels']\n",
    "    }\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_datasets = hf_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Display the first few processed entries to verify\n",
    "print(tokenized_datasets.select(range(2)))  # Select the first two entries for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 24 18:03:20 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 PCIe               Off | 00000000:B5:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              53W / 350W |      4MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model state dictionary\n",
    "model_path = 'transformer_dataset2.pth'\n",
    "loaded_state_dict = torch.load(model_path)\n",
    "# Create a new instance of the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33901' max='56820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33901/56820 1:53:16 < 1:16:34, 4.99 it/s, Epoch 5.97/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.246300</td>\n",
       "      <td>0.341686</td>\n",
       "      <td>0.910067</td>\n",
       "      <td>0.903676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.118100</td>\n",
       "      <td>0.278590</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.915551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.302400</td>\n",
       "      <td>0.244884</td>\n",
       "      <td>0.926786</td>\n",
       "      <td>0.924004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.453486</td>\n",
       "      <td>0.911475</td>\n",
       "      <td>0.906766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.390033</td>\n",
       "      <td>0.895459</td>\n",
       "      <td>0.887840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.333033</td>\n",
       "      <td>0.883844</td>\n",
       "      <td>0.876774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>1.087824</td>\n",
       "      <td>0.782999</td>\n",
       "      <td>0.728474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.489211</td>\n",
       "      <td>0.877684</td>\n",
       "      <td>0.867745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>0.388674</td>\n",
       "      <td>0.887364</td>\n",
       "      <td>0.889693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.253435</td>\n",
       "      <td>0.912355</td>\n",
       "      <td>0.913602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>0.322939</td>\n",
       "      <td>0.910419</td>\n",
       "      <td>0.908139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.326300</td>\n",
       "      <td>0.459952</td>\n",
       "      <td>0.883668</td>\n",
       "      <td>0.875024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.331743</td>\n",
       "      <td>0.914291</td>\n",
       "      <td>0.916251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.372613</td>\n",
       "      <td>0.906899</td>\n",
       "      <td>0.906554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.598089</td>\n",
       "      <td>0.847061</td>\n",
       "      <td>0.823050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.702900</td>\n",
       "      <td>0.363427</td>\n",
       "      <td>0.912531</td>\n",
       "      <td>0.911739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.550037</td>\n",
       "      <td>0.879268</td>\n",
       "      <td>0.868733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.402659</td>\n",
       "      <td>0.912179</td>\n",
       "      <td>0.909322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.437585</td>\n",
       "      <td>0.899331</td>\n",
       "      <td>0.894192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.248600</td>\n",
       "      <td>0.411975</td>\n",
       "      <td>0.880852</td>\n",
       "      <td>0.871072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.265710</td>\n",
       "      <td>0.916051</td>\n",
       "      <td>0.914958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.363923</td>\n",
       "      <td>0.908483</td>\n",
       "      <td>0.903166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.314475</td>\n",
       "      <td>0.922386</td>\n",
       "      <td>0.920583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>0.452788</td>\n",
       "      <td>0.909187</td>\n",
       "      <td>0.904937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.298700</td>\n",
       "      <td>0.413515</td>\n",
       "      <td>0.907075</td>\n",
       "      <td>0.901603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>0.338349</td>\n",
       "      <td>0.918691</td>\n",
       "      <td>0.915323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.341427</td>\n",
       "      <td>0.910243</td>\n",
       "      <td>0.905486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>0.451263</td>\n",
       "      <td>0.891411</td>\n",
       "      <td>0.883166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.355297</td>\n",
       "      <td>0.913059</td>\n",
       "      <td>0.909358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>0.301264</td>\n",
       "      <td>0.927842</td>\n",
       "      <td>0.927715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.546125</td>\n",
       "      <td>0.898275</td>\n",
       "      <td>0.890696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.114200</td>\n",
       "      <td>0.410416</td>\n",
       "      <td>0.908131</td>\n",
       "      <td>0.902902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.202300</td>\n",
       "      <td>0.386328</td>\n",
       "      <td>0.917459</td>\n",
       "      <td>0.914990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.446273</td>\n",
       "      <td>0.906371</td>\n",
       "      <td>0.901042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.294725</td>\n",
       "      <td>0.929426</td>\n",
       "      <td>0.928635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.129300</td>\n",
       "      <td>0.247693</td>\n",
       "      <td>0.922210</td>\n",
       "      <td>0.918480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>0.620426</td>\n",
       "      <td>0.857797</td>\n",
       "      <td>0.837686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.319619</td>\n",
       "      <td>0.925554</td>\n",
       "      <td>0.923328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.302500</td>\n",
       "      <td>0.392513</td>\n",
       "      <td>0.905315</td>\n",
       "      <td>0.899963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.265902</td>\n",
       "      <td>0.931010</td>\n",
       "      <td>0.932762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.381733</td>\n",
       "      <td>0.928370</td>\n",
       "      <td>0.925280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.172600</td>\n",
       "      <td>0.184695</td>\n",
       "      <td>0.948610</td>\n",
       "      <td>0.948826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.494185</td>\n",
       "      <td>0.862548</td>\n",
       "      <td>0.843141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.250730</td>\n",
       "      <td>0.940514</td>\n",
       "      <td>0.940135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.092500</td>\n",
       "      <td>0.232270</td>\n",
       "      <td>0.942274</td>\n",
       "      <td>0.943135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.201231</td>\n",
       "      <td>0.942274</td>\n",
       "      <td>0.941554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.238413</td>\n",
       "      <td>0.937874</td>\n",
       "      <td>0.937311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.240428</td>\n",
       "      <td>0.938050</td>\n",
       "      <td>0.938332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>0.190707</td>\n",
       "      <td>0.942098</td>\n",
       "      <td>0.942872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.321830</td>\n",
       "      <td>0.925026</td>\n",
       "      <td>0.922517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.215300</td>\n",
       "      <td>0.206192</td>\n",
       "      <td>0.944034</td>\n",
       "      <td>0.943737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.385800</td>\n",
       "      <td>0.214850</td>\n",
       "      <td>0.937874</td>\n",
       "      <td>0.936362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>0.236184</td>\n",
       "      <td>0.936114</td>\n",
       "      <td>0.934203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>0.400900</td>\n",
       "      <td>0.910067</td>\n",
       "      <td>0.903421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.301700</td>\n",
       "      <td>0.341320</td>\n",
       "      <td>0.884020</td>\n",
       "      <td>0.870607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.218916</td>\n",
       "      <td>0.947026</td>\n",
       "      <td>0.945873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.266881</td>\n",
       "      <td>0.932946</td>\n",
       "      <td>0.929873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.078200</td>\n",
       "      <td>0.284121</td>\n",
       "      <td>0.944562</td>\n",
       "      <td>0.943089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.257008</td>\n",
       "      <td>0.943330</td>\n",
       "      <td>0.941155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.263439</td>\n",
       "      <td>0.944210</td>\n",
       "      <td>0.942479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.343146</td>\n",
       "      <td>0.936290</td>\n",
       "      <td>0.934657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.199600</td>\n",
       "      <td>0.336344</td>\n",
       "      <td>0.930658</td>\n",
       "      <td>0.927010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.571335</td>\n",
       "      <td>0.897747</td>\n",
       "      <td>0.889565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.372183</td>\n",
       "      <td>0.927666</td>\n",
       "      <td>0.924877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.460874</td>\n",
       "      <td>0.922034</td>\n",
       "      <td>0.917243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.219193</td>\n",
       "      <td>0.951954</td>\n",
       "      <td>0.951049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.256286</td>\n",
       "      <td>0.939986</td>\n",
       "      <td>0.937649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>0.297646</td>\n",
       "      <td>0.937170</td>\n",
       "      <td>0.936033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.293900</td>\n",
       "      <td>0.327251</td>\n",
       "      <td>0.936994</td>\n",
       "      <td>0.934791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.417521</td>\n",
       "      <td>0.924146</td>\n",
       "      <td>0.919903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.404231</td>\n",
       "      <td>0.930306</td>\n",
       "      <td>0.926802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.436013</td>\n",
       "      <td>0.904963</td>\n",
       "      <td>0.896789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.331466</td>\n",
       "      <td>0.928018</td>\n",
       "      <td>0.924329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>0.206337</td>\n",
       "      <td>0.949138</td>\n",
       "      <td>0.948384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.253100</td>\n",
       "      <td>0.251160</td>\n",
       "      <td>0.939106</td>\n",
       "      <td>0.936792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.327632</td>\n",
       "      <td>0.931538</td>\n",
       "      <td>0.928611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.220335</td>\n",
       "      <td>0.954065</td>\n",
       "      <td>0.953797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.345681</td>\n",
       "      <td>0.939106</td>\n",
       "      <td>0.936999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.951250</td>\n",
       "      <td>0.951189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.315500</td>\n",
       "      <td>0.240478</td>\n",
       "      <td>0.953889</td>\n",
       "      <td>0.954035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.306148</td>\n",
       "      <td>0.931538</td>\n",
       "      <td>0.928479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.294011</td>\n",
       "      <td>0.945090</td>\n",
       "      <td>0.944186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.250960</td>\n",
       "      <td>0.945618</td>\n",
       "      <td>0.944153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.314482</td>\n",
       "      <td>0.934354</td>\n",
       "      <td>0.931547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.229059</td>\n",
       "      <td>0.936818</td>\n",
       "      <td>0.934644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.190100</td>\n",
       "      <td>0.207290</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.955312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.274815</td>\n",
       "      <td>0.945794</td>\n",
       "      <td>0.945756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.218997</td>\n",
       "      <td>0.947378</td>\n",
       "      <td>0.945941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.315155</td>\n",
       "      <td>0.939810</td>\n",
       "      <td>0.937863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.162400</td>\n",
       "      <td>0.307579</td>\n",
       "      <td>0.934178</td>\n",
       "      <td>0.932951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.217506</td>\n",
       "      <td>0.953185</td>\n",
       "      <td>0.952803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.319532</td>\n",
       "      <td>0.938930</td>\n",
       "      <td>0.936829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.227912</td>\n",
       "      <td>0.944386</td>\n",
       "      <td>0.943145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.917283</td>\n",
       "      <td>0.912248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.296262</td>\n",
       "      <td>0.938226</td>\n",
       "      <td>0.935703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.227056</td>\n",
       "      <td>0.950546</td>\n",
       "      <td>0.949759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.116900</td>\n",
       "      <td>0.228576</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.952193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.311512</td>\n",
       "      <td>0.940162</td>\n",
       "      <td>0.942157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.228470</td>\n",
       "      <td>0.947202</td>\n",
       "      <td>0.945750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.216587</td>\n",
       "      <td>0.951954</td>\n",
       "      <td>0.951587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.358190</td>\n",
       "      <td>0.934178</td>\n",
       "      <td>0.931149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.365396</td>\n",
       "      <td>0.934178</td>\n",
       "      <td>0.930971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.203467</td>\n",
       "      <td>0.956177</td>\n",
       "      <td>0.955670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>0.400161</td>\n",
       "      <td>0.911651</td>\n",
       "      <td>0.904996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.255016</td>\n",
       "      <td>0.949138</td>\n",
       "      <td>0.948640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.326968</td>\n",
       "      <td>0.936114</td>\n",
       "      <td>0.933161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.240394</td>\n",
       "      <td>0.947026</td>\n",
       "      <td>0.945619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.236156</td>\n",
       "      <td>0.950722</td>\n",
       "      <td>0.949385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>0.315387</td>\n",
       "      <td>0.940514</td>\n",
       "      <td>0.938186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.254332</td>\n",
       "      <td>0.947378</td>\n",
       "      <td>0.945488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.164800</td>\n",
       "      <td>0.274520</td>\n",
       "      <td>0.934882</td>\n",
       "      <td>0.931935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.128200</td>\n",
       "      <td>0.294381</td>\n",
       "      <td>0.947906</td>\n",
       "      <td>0.946474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.204300</td>\n",
       "      <td>0.220534</td>\n",
       "      <td>0.951250</td>\n",
       "      <td>0.950189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.217262</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.954295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.268315</td>\n",
       "      <td>0.950018</td>\n",
       "      <td>0.948569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.288472</td>\n",
       "      <td>0.956881</td>\n",
       "      <td>0.956382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.240428</td>\n",
       "      <td>0.956705</td>\n",
       "      <td>0.956087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.388334</td>\n",
       "      <td>0.926082</td>\n",
       "      <td>0.921904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.286398</td>\n",
       "      <td>0.950194</td>\n",
       "      <td>0.948963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.294005</td>\n",
       "      <td>0.945442</td>\n",
       "      <td>0.943779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.348370</td>\n",
       "      <td>0.937346</td>\n",
       "      <td>0.934607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.278976</td>\n",
       "      <td>0.953010</td>\n",
       "      <td>0.954147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.266025</td>\n",
       "      <td>0.955297</td>\n",
       "      <td>0.954464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>0.338986</td>\n",
       "      <td>0.943506</td>\n",
       "      <td>0.941263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.304070</td>\n",
       "      <td>0.944386</td>\n",
       "      <td>0.942504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.287224</td>\n",
       "      <td>0.951602</td>\n",
       "      <td>0.950761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.269995</td>\n",
       "      <td>0.955121</td>\n",
       "      <td>0.954472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.295923</td>\n",
       "      <td>0.957057</td>\n",
       "      <td>0.957042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.284371</td>\n",
       "      <td>0.947026</td>\n",
       "      <td>0.945501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>0.229047</td>\n",
       "      <td>0.958113</td>\n",
       "      <td>0.957921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.271941</td>\n",
       "      <td>0.951602</td>\n",
       "      <td>0.951847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>0.269788</td>\n",
       "      <td>0.945090</td>\n",
       "      <td>0.943580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.272229</td>\n",
       "      <td>0.950546</td>\n",
       "      <td>0.949488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.303538</td>\n",
       "      <td>0.953537</td>\n",
       "      <td>0.953521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.298916</td>\n",
       "      <td>0.958289</td>\n",
       "      <td>0.958238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.302752</td>\n",
       "      <td>0.950018</td>\n",
       "      <td>0.949304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.217351</td>\n",
       "      <td>0.948082</td>\n",
       "      <td>0.946741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.307905</td>\n",
       "      <td>0.945266</td>\n",
       "      <td>0.943465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.320655</td>\n",
       "      <td>0.941922</td>\n",
       "      <td>0.939472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.357156</td>\n",
       "      <td>0.931538</td>\n",
       "      <td>0.927816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.369220</td>\n",
       "      <td>0.932066</td>\n",
       "      <td>0.928492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.243429</td>\n",
       "      <td>0.956001</td>\n",
       "      <td>0.955642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.232576</td>\n",
       "      <td>0.955121</td>\n",
       "      <td>0.955019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.353126</td>\n",
       "      <td>0.943330</td>\n",
       "      <td>0.941305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.321564</td>\n",
       "      <td>0.942626</td>\n",
       "      <td>0.940446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>0.272658</td>\n",
       "      <td>0.942626</td>\n",
       "      <td>0.940813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.382355</td>\n",
       "      <td>0.944034</td>\n",
       "      <td>0.942433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.284019</td>\n",
       "      <td>0.943154</td>\n",
       "      <td>0.941475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.369190</td>\n",
       "      <td>0.940514</td>\n",
       "      <td>0.938456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.448805</td>\n",
       "      <td>0.920979</td>\n",
       "      <td>0.915933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.302396</td>\n",
       "      <td>0.948258</td>\n",
       "      <td>0.947387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.381663</td>\n",
       "      <td>0.931186</td>\n",
       "      <td>0.927873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.300487</td>\n",
       "      <td>0.945618</td>\n",
       "      <td>0.945086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.262649</td>\n",
       "      <td>0.939458</td>\n",
       "      <td>0.937295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.298002</td>\n",
       "      <td>0.945266</td>\n",
       "      <td>0.943731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.330598</td>\n",
       "      <td>0.947906</td>\n",
       "      <td>0.946915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.278896</td>\n",
       "      <td>0.952658</td>\n",
       "      <td>0.952616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.461757</td>\n",
       "      <td>0.924498</td>\n",
       "      <td>0.919978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.248368</td>\n",
       "      <td>0.946146</td>\n",
       "      <td>0.944705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.416756</td>\n",
       "      <td>0.930130</td>\n",
       "      <td>0.926901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>0.220662</td>\n",
       "      <td>0.961105</td>\n",
       "      <td>0.961180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.296424</td>\n",
       "      <td>0.952834</td>\n",
       "      <td>0.952228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.320038</td>\n",
       "      <td>0.953185</td>\n",
       "      <td>0.952517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.329943</td>\n",
       "      <td>0.944386</td>\n",
       "      <td>0.942940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.304021</td>\n",
       "      <td>0.949314</td>\n",
       "      <td>0.948608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.191900</td>\n",
       "      <td>0.281195</td>\n",
       "      <td>0.947202</td>\n",
       "      <td>0.945711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.292353</td>\n",
       "      <td>0.948962</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.284438</td>\n",
       "      <td>0.944914</td>\n",
       "      <td>0.942914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.265302</td>\n",
       "      <td>0.957409</td>\n",
       "      <td>0.956832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.285044</td>\n",
       "      <td>0.946674</td>\n",
       "      <td>0.945119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.389045</td>\n",
       "      <td>0.932418</td>\n",
       "      <td>0.929360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.380202</td>\n",
       "      <td>0.937522</td>\n",
       "      <td>0.934610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.265308</td>\n",
       "      <td>0.958465</td>\n",
       "      <td>0.958538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.340164</td>\n",
       "      <td>0.952658</td>\n",
       "      <td>0.951610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.471801</td>\n",
       "      <td>0.929426</td>\n",
       "      <td>0.925534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.352567</td>\n",
       "      <td>0.942978</td>\n",
       "      <td>0.940703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.249840</td>\n",
       "      <td>0.959521</td>\n",
       "      <td>0.959002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>0.287999</td>\n",
       "      <td>0.954065</td>\n",
       "      <td>0.953217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.257046</td>\n",
       "      <td>0.958113</td>\n",
       "      <td>0.957545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.398451</td>\n",
       "      <td>0.943506</td>\n",
       "      <td>0.941541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.344833</td>\n",
       "      <td>0.950546</td>\n",
       "      <td>0.949669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.322708</td>\n",
       "      <td>0.949490</td>\n",
       "      <td>0.947998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.280348</td>\n",
       "      <td>0.956353</td>\n",
       "      <td>0.955777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.223805</td>\n",
       "      <td>0.964449</td>\n",
       "      <td>0.964336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.260422</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.963145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.256400</td>\n",
       "      <td>0.958817</td>\n",
       "      <td>0.958259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>0.221606</td>\n",
       "      <td>0.960225</td>\n",
       "      <td>0.959915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.354850</td>\n",
       "      <td>0.948786</td>\n",
       "      <td>0.947139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.340239</td>\n",
       "      <td>0.953713</td>\n",
       "      <td>0.952859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.314385</td>\n",
       "      <td>0.951954</td>\n",
       "      <td>0.950961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.274267</td>\n",
       "      <td>0.954945</td>\n",
       "      <td>0.954610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.302112</td>\n",
       "      <td>0.947730</td>\n",
       "      <td>0.946535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.303522</td>\n",
       "      <td>0.953361</td>\n",
       "      <td>0.952687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.955825</td>\n",
       "      <td>0.955425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.311617</td>\n",
       "      <td>0.949490</td>\n",
       "      <td>0.948242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.437922</td>\n",
       "      <td>0.937346</td>\n",
       "      <td>0.934703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.390812</td>\n",
       "      <td>0.939458</td>\n",
       "      <td>0.936904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.085400</td>\n",
       "      <td>0.342898</td>\n",
       "      <td>0.943858</td>\n",
       "      <td>0.942283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.326511</td>\n",
       "      <td>0.950722</td>\n",
       "      <td>0.950231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.564525</td>\n",
       "      <td>0.916755</td>\n",
       "      <td>0.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>0.337282</td>\n",
       "      <td>0.951954</td>\n",
       "      <td>0.950908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.422310</td>\n",
       "      <td>0.922738</td>\n",
       "      <td>0.917497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.457705</td>\n",
       "      <td>0.935938</td>\n",
       "      <td>0.932841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.349978</td>\n",
       "      <td>0.949138</td>\n",
       "      <td>0.948273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.329581</td>\n",
       "      <td>0.953361</td>\n",
       "      <td>0.952364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.277896</td>\n",
       "      <td>0.949842</td>\n",
       "      <td>0.948824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.334284</td>\n",
       "      <td>0.952834</td>\n",
       "      <td>0.951902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>0.232982</td>\n",
       "      <td>0.955649</td>\n",
       "      <td>0.955944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.377468</td>\n",
       "      <td>0.949138</td>\n",
       "      <td>0.947937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.433161</td>\n",
       "      <td>0.945618</td>\n",
       "      <td>0.943767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.354202</td>\n",
       "      <td>0.950898</td>\n",
       "      <td>0.949991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.401831</td>\n",
       "      <td>0.937874</td>\n",
       "      <td>0.935360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.231638</td>\n",
       "      <td>0.949842</td>\n",
       "      <td>0.948934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.433424</td>\n",
       "      <td>0.940162</td>\n",
       "      <td>0.938024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.309910</td>\n",
       "      <td>0.939986</td>\n",
       "      <td>0.937489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.327182</td>\n",
       "      <td>0.945618</td>\n",
       "      <td>0.943930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.325744</td>\n",
       "      <td>0.947906</td>\n",
       "      <td>0.946551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.223246</td>\n",
       "      <td>0.958641</td>\n",
       "      <td>0.958808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355875</td>\n",
       "      <td>0.951426</td>\n",
       "      <td>0.950591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.404259</td>\n",
       "      <td>0.950546</td>\n",
       "      <td>0.949524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.389787</td>\n",
       "      <td>0.952834</td>\n",
       "      <td>0.952347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>0.374268</td>\n",
       "      <td>0.941746</td>\n",
       "      <td>0.939543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.294166</td>\n",
       "      <td>0.956001</td>\n",
       "      <td>0.955877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.225904</td>\n",
       "      <td>0.960577</td>\n",
       "      <td>0.960619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.264080</td>\n",
       "      <td>0.958465</td>\n",
       "      <td>0.957932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.314029</td>\n",
       "      <td>0.954945</td>\n",
       "      <td>0.954253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.304923</td>\n",
       "      <td>0.957761</td>\n",
       "      <td>0.957356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.298276</td>\n",
       "      <td>0.960929</td>\n",
       "      <td>0.960998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315526</td>\n",
       "      <td>0.957409</td>\n",
       "      <td>0.957603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.325676</td>\n",
       "      <td>0.958113</td>\n",
       "      <td>0.957801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.332320</td>\n",
       "      <td>0.956881</td>\n",
       "      <td>0.956537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.355647</td>\n",
       "      <td>0.953361</td>\n",
       "      <td>0.952534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342734</td>\n",
       "      <td>0.953889</td>\n",
       "      <td>0.953314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.309525</td>\n",
       "      <td>0.960049</td>\n",
       "      <td>0.960252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436009</td>\n",
       "      <td>0.945266</td>\n",
       "      <td>0.943506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338932</td>\n",
       "      <td>0.955121</td>\n",
       "      <td>0.955003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.466429</td>\n",
       "      <td>0.930306</td>\n",
       "      <td>0.926612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.341653</td>\n",
       "      <td>0.946146</td>\n",
       "      <td>0.944364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.313578</td>\n",
       "      <td>0.955121</td>\n",
       "      <td>0.954342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.480672</td>\n",
       "      <td>0.933650</td>\n",
       "      <td>0.930276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404648</td>\n",
       "      <td>0.946146</td>\n",
       "      <td>0.944525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.649459</td>\n",
       "      <td>0.920627</td>\n",
       "      <td>0.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.318710</td>\n",
       "      <td>0.956529</td>\n",
       "      <td>0.956182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.431724</td>\n",
       "      <td>0.939458</td>\n",
       "      <td>0.937180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367875</td>\n",
       "      <td>0.953537</td>\n",
       "      <td>0.952975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.276482</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.954230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466175</td>\n",
       "      <td>0.939282</td>\n",
       "      <td>0.936778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.388641</td>\n",
       "      <td>0.945442</td>\n",
       "      <td>0.943575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.319531</td>\n",
       "      <td>0.956353</td>\n",
       "      <td>0.956075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.094200</td>\n",
       "      <td>0.310871</td>\n",
       "      <td>0.951954</td>\n",
       "      <td>0.951084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.442721</td>\n",
       "      <td>0.941042</td>\n",
       "      <td>0.938678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.329950</td>\n",
       "      <td>0.954065</td>\n",
       "      <td>0.953550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.456038</td>\n",
       "      <td>0.936818</td>\n",
       "      <td>0.934309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.274506</td>\n",
       "      <td>0.957761</td>\n",
       "      <td>0.957341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.268261</td>\n",
       "      <td>0.962337</td>\n",
       "      <td>0.962151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.291949</td>\n",
       "      <td>0.954417</td>\n",
       "      <td>0.953659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.281579</td>\n",
       "      <td>0.955825</td>\n",
       "      <td>0.955187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.581175</td>\n",
       "      <td>0.923970</td>\n",
       "      <td>0.919583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25900</td>\n",
       "      <td>0.106800</td>\n",
       "      <td>0.295450</td>\n",
       "      <td>0.958113</td>\n",
       "      <td>0.957906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>0.472891</td>\n",
       "      <td>0.938930</td>\n",
       "      <td>0.936155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.284447</td>\n",
       "      <td>0.956881</td>\n",
       "      <td>0.956413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.270932</td>\n",
       "      <td>0.957057</td>\n",
       "      <td>0.956304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26300</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.314472</td>\n",
       "      <td>0.951426</td>\n",
       "      <td>0.950054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257385</td>\n",
       "      <td>0.963217</td>\n",
       "      <td>0.963442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336408</td>\n",
       "      <td>0.950898</td>\n",
       "      <td>0.949630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.402212</td>\n",
       "      <td>0.947730</td>\n",
       "      <td>0.946205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.380327</td>\n",
       "      <td>0.948082</td>\n",
       "      <td>0.946741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.261572</td>\n",
       "      <td>0.955121</td>\n",
       "      <td>0.955145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26900</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.283709</td>\n",
       "      <td>0.954241</td>\n",
       "      <td>0.953472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.304372</td>\n",
       "      <td>0.956177</td>\n",
       "      <td>0.955718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.434171</td>\n",
       "      <td>0.945442</td>\n",
       "      <td>0.944064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.270143</td>\n",
       "      <td>0.958113</td>\n",
       "      <td>0.958231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>0.302660</td>\n",
       "      <td>0.956529</td>\n",
       "      <td>0.956229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.331474</td>\n",
       "      <td>0.953361</td>\n",
       "      <td>0.952687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.324106</td>\n",
       "      <td>0.947730</td>\n",
       "      <td>0.946380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.395634</td>\n",
       "      <td>0.947026</td>\n",
       "      <td>0.945322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.317409</td>\n",
       "      <td>0.956529</td>\n",
       "      <td>0.955790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.415076</td>\n",
       "      <td>0.949314</td>\n",
       "      <td>0.947883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.386276</td>\n",
       "      <td>0.950194</td>\n",
       "      <td>0.949183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.425599</td>\n",
       "      <td>0.944034</td>\n",
       "      <td>0.941843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.294252</td>\n",
       "      <td>0.960753</td>\n",
       "      <td>0.960341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.375178</td>\n",
       "      <td>0.956177</td>\n",
       "      <td>0.955192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.249786</td>\n",
       "      <td>0.964449</td>\n",
       "      <td>0.964722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.276541</td>\n",
       "      <td>0.961809</td>\n",
       "      <td>0.961613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455958</td>\n",
       "      <td>0.938578</td>\n",
       "      <td>0.936045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.419734</td>\n",
       "      <td>0.946850</td>\n",
       "      <td>0.945171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300539</td>\n",
       "      <td>0.958993</td>\n",
       "      <td>0.958914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342252</td>\n",
       "      <td>0.957585</td>\n",
       "      <td>0.956895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.401891</td>\n",
       "      <td>0.948786</td>\n",
       "      <td>0.947349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.524735</td>\n",
       "      <td>0.932418</td>\n",
       "      <td>0.928863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.302381</td>\n",
       "      <td>0.951778</td>\n",
       "      <td>0.950843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280902</td>\n",
       "      <td>0.957937</td>\n",
       "      <td>0.957781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314177</td>\n",
       "      <td>0.957761</td>\n",
       "      <td>0.957173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.375126</td>\n",
       "      <td>0.953361</td>\n",
       "      <td>0.952175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.436239</td>\n",
       "      <td>0.947730</td>\n",
       "      <td>0.945971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346678</td>\n",
       "      <td>0.960401</td>\n",
       "      <td>0.959829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.305732</td>\n",
       "      <td>0.963569</td>\n",
       "      <td>0.963855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.288998</td>\n",
       "      <td>0.957409</td>\n",
       "      <td>0.956832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29900</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.327673</td>\n",
       "      <td>0.951954</td>\n",
       "      <td>0.950979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.338052</td>\n",
       "      <td>0.949842</td>\n",
       "      <td>0.948750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544565</td>\n",
       "      <td>0.935234</td>\n",
       "      <td>0.932675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653325</td>\n",
       "      <td>0.916051</td>\n",
       "      <td>0.909608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288763</td>\n",
       "      <td>0.961633</td>\n",
       "      <td>0.961402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.395848</td>\n",
       "      <td>0.945266</td>\n",
       "      <td>0.943259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.450058</td>\n",
       "      <td>0.942802</td>\n",
       "      <td>0.940618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.386910</td>\n",
       "      <td>0.954065</td>\n",
       "      <td>0.953234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380297</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.955940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498494</td>\n",
       "      <td>0.945618</td>\n",
       "      <td>0.943910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.357550</td>\n",
       "      <td>0.958465</td>\n",
       "      <td>0.958363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439963</td>\n",
       "      <td>0.945266</td>\n",
       "      <td>0.943608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.298416</td>\n",
       "      <td>0.960049</td>\n",
       "      <td>0.960042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.377571</td>\n",
       "      <td>0.946498</td>\n",
       "      <td>0.944928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312954</td>\n",
       "      <td>0.955473</td>\n",
       "      <td>0.954878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.402559</td>\n",
       "      <td>0.946674</td>\n",
       "      <td>0.945119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.415153</td>\n",
       "      <td>0.948962</td>\n",
       "      <td>0.947483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376598</td>\n",
       "      <td>0.956529</td>\n",
       "      <td>0.956182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.370090</td>\n",
       "      <td>0.945442</td>\n",
       "      <td>0.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.439543</td>\n",
       "      <td>0.942626</td>\n",
       "      <td>0.940467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31900</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.329728</td>\n",
       "      <td>0.957409</td>\n",
       "      <td>0.956909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.278065</td>\n",
       "      <td>0.955473</td>\n",
       "      <td>0.955324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.322498</td>\n",
       "      <td>0.958641</td>\n",
       "      <td>0.958163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307048</td>\n",
       "      <td>0.962337</td>\n",
       "      <td>0.962337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32300</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.458220</td>\n",
       "      <td>0.943506</td>\n",
       "      <td>0.941241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.381006</td>\n",
       "      <td>0.956705</td>\n",
       "      <td>0.955787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353174</td>\n",
       "      <td>0.949138</td>\n",
       "      <td>0.947956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.288959</td>\n",
       "      <td>0.957585</td>\n",
       "      <td>0.957018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281136</td>\n",
       "      <td>0.962689</td>\n",
       "      <td>0.962729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314960</td>\n",
       "      <td>0.960225</td>\n",
       "      <td>0.959844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311471</td>\n",
       "      <td>0.955121</td>\n",
       "      <td>0.954293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.303922</td>\n",
       "      <td>0.960929</td>\n",
       "      <td>0.960624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.287221</td>\n",
       "      <td>0.963217</td>\n",
       "      <td>0.963442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284188</td>\n",
       "      <td>0.962513</td>\n",
       "      <td>0.962374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343844</td>\n",
       "      <td>0.959521</td>\n",
       "      <td>0.959016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>0.951250</td>\n",
       "      <td>0.949973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.479697</td>\n",
       "      <td>0.943154</td>\n",
       "      <td>0.940983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404246</td>\n",
       "      <td>0.950370</td>\n",
       "      <td>0.948969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449406</td>\n",
       "      <td>0.945970</td>\n",
       "      <td>0.944009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.268577</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.962819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [356/356 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/Hackathon-AI-vs-IA/distilbert.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://user-assil-92139-0.user.lab.sspcloud.fr/home/onyxia/Hackathon-AI-vs-IA/distilbert.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://user-assil-92139-0.user.lab.sspcloud.fr/home/onyxia/Hackathon-AI-vs-IA/distilbert.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://user-assil-92139-0.user.lab.sspcloud.fr/home/onyxia/Hackathon-AI-vs-IA/distilbert.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://user-assil-92139-0.user.lab.sspcloud.fr/home/onyxia/Hackathon-AI-vs-IA/distilbert.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m     <a href='vscode-notebook-cell://user-assil-92139-0.user.lab.sspcloud.fr/home/onyxia/Hackathon-AI-vs-IA/distilbert.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://user-assil-92139-0.user.lab.sspcloud.fr/home/onyxia/Hackathon-AI-vs-IA/distilbert.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-assil-92139-0.user.lab.sspcloud.fr/home/onyxia/Hackathon-AI-vs-IA/distilbert.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://user-assil-92139-0.user.lab.sspcloud.fr/home/onyxia/Hackathon-AI-vs-IA/distilbert.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Save the trained model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://user-assil-92139-0.user.lab.sspcloud.fr/home/onyxia/Hackathon-AI-vs-IA/distilbert.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mmodel_distilbert2.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1860\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1861\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1862\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1863\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1864\u001b[0m     )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/trainer.py:2278\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2275\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2276\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 2278\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/trainer.py:2662\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2660\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2661\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2662\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2663\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2665\u001b[0m     \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/trainer.py:3467\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3464\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3466\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3467\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3468\u001b[0m     eval_dataloader,\n\u001b[1;32m   3469\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   3470\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3471\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3472\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   3473\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   3474\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   3475\u001b[0m )\n\u001b[1;32m   3477\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/transformers/trainer.py:3666\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3664\u001b[0m     all_inputs\u001b[39m.\u001b[39madd(inputs_decode)\n\u001b[1;32m   3665\u001b[0m \u001b[39mif\u001b[39;00m logits \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 3666\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mpad_across_processes(logits, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, pad_index\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m   3667\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_logits_for_metrics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3668\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/accelerate/accelerator.py:2350\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[0;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m   2317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpad_across_processes\u001b[39m(\u001b[39mself\u001b[39m, tensor, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, pad_index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, pad_first\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   2318\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2319\u001b[0m \u001b[39m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[1;32m   2320\u001b[0m \u001b[39m    they can safely be gathered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m   2349\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2350\u001b[0m     \u001b[39mreturn\u001b[39;00m pad_across_processes(tensor, dim\u001b[39m=\u001b[39;49mdim, pad_index\u001b[39m=\u001b[39;49mpad_index, pad_first\u001b[39m=\u001b[39;49mpad_first)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/accelerate/utils/operations.py:417\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m@wraps\u001b[39m(function)\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    416\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m         \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    418\u001b[0m     \u001b[39mexcept\u001b[39;00m DistributedOperationException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    419\u001b[0m         operation \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunction\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mfunction\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/accelerate/utils/operations.py:684\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    681\u001b[0m     new_tensor[indices] \u001b[39m=\u001b[39m tensor\n\u001b[1;32m    682\u001b[0m     \u001b[39mreturn\u001b[39;00m new_tensor\n\u001b[0;32m--> 684\u001b[0m \u001b[39mreturn\u001b[39;00m recursively_apply(\n\u001b[1;32m    685\u001b[0m     _pad_across_processes, tensor, error_on_other_type\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, dim\u001b[39m=\u001b[39;49mdim, pad_index\u001b[39m=\u001b[39;49mpad_index, pad_first\u001b[39m=\u001b[39;49mpad_first\n\u001b[1;32m    686\u001b[0m )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/accelerate/utils/operations.py:126\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\n\u001b[1;32m    118\u001b[0m         {\n\u001b[1;32m    119\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         }\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[39melif\u001b[39;00m test_type(data):\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m func(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    127\u001b[0m \u001b[39melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m    128\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported types (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) passed to `\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m`. Only nested list/tuple/dicts of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mobjects that are valid for `\u001b[39m\u001b[39m{\u001b[39;00mtest_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` should be passed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m     )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/accelerate/utils/operations.py:664\u001b[0m, in \u001b[0;36mpad_across_processes.<locals>._pad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\n\u001b[1;32m    663\u001b[0m \u001b[39m# Gather all sizes\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m size \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(tensor\u001b[39m.\u001b[39;49mshape, device\u001b[39m=\u001b[39;49mtensor\u001b[39m.\u001b[39;49mdevice)[\u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    665\u001b[0m sizes \u001b[39m=\u001b[39m gather(size)\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    666\u001b[0m \u001b[39m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "# Split the dataset\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_val_dataset = train_test_split['test']\n",
    "\n",
    "test_val_split = test_val_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "eval_dataset = test_val_split['train']\n",
    "test_dataset = test_val_split['test']\n",
    "\n",
    "# Check for GPU availability and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model.device)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100\n",
    ")\n",
    "\n",
    "# Define the metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "    }\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model_distilbert2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_distilbert2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9598732840549102\n",
      "Test F1 Score: 0.9604578563995838\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Evaluate the model on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "test_metrics = compute_metrics(predictions)\n",
    "print(\"Test Accuracy:\", test_metrics['accuracy'])\n",
    "print(\"Test F1 Score:\", test_metrics['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "0 Precision: 0.9388\n",
      "0 Recall: 0.9806\n",
      "0 F1-score: 0.9593\n",
      "0 Support: 2738.0000\n",
      "1 Precision: 0.9812\n",
      "1 Recall: 0.9406\n",
      "1 F1-score: 0.9605\n",
      "1 Support: 2944.0000\n",
      "Accuracy: 0.9599\n",
      "Macro avg Precision: 0.9600\n",
      "Macro avg Recall: 0.9606\n",
      "Macro avg F1-score: 0.9599\n",
      "Macro avg Support: 5682.0000\n",
      "Weighted avg Precision: 0.9608\n",
      "Weighted avg Recall: 0.9599\n",
      "Weighted avg F1-score: 0.9599\n",
      "Weighted avg Support: 5682.0000\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming predictions have been made and are stored in 'predictions'\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# If compute_metrics is a function you define to calculate metrics, it should return a dictionary\n",
    "def compute_metrics(pred):\n",
    "    y_true = pred.label_ids\n",
    "    y_pred = pred.predictions.argmax(-1)\n",
    "    return classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "test_metrics = compute_metrics(predictions)\n",
    "\n",
    "# Print all metrics\n",
    "print(\"Test Metrics:\")\n",
    "for metric in test_metrics:\n",
    "    if isinstance(test_metrics[metric], dict):  # This handles metrics which are dictionaries themselves, like precision, recall, f1-score for each class\n",
    "        for sub_metric in test_metrics[metric]:\n",
    "            print(f\"{metric.capitalize()} {sub_metric.capitalize()}: {test_metrics[metric][sub_metric]:.4f}\")\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: {test_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability and set the device accordingly\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cuda\")\n",
    "model.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Function to make a prediction on a single sentence\n",
    "def predict(sentence):\n",
    "    # Tokenize the sentence so it matches the format expected by the model\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():  # Disable gradient calculation to speed up the process and reduce memory usage\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Apply softmax to logits to get probabilities\n",
    "    probabilities = softmax(logits, dim=1)\n",
    "\n",
    "    # Assuming we have two classes, 0 and 1, and class 0 is the 'negative' class\n",
    "    prediction = probabilities.argmax().item()  # Get the index of the highest probability\n",
    "    return {\"class\": prediction, \"probabilities\": probabilities.tolist()[0]}\n",
    "\n",
    "# Example usage\n",
    "user_sentence = \"Yes, it is possible to be subject to a cash withdrawal even if you do not use an ATM. There are several ways that this could happen:Debit card transactions: If you make a purchase using your debit card, the merchant may automatically withdraw the amount of the purchase from your checking account. This is essentially the same as making a cash withdrawal.Bank fees: Some banks charge fees for maintaining an account or for using certain services. These fees may be automatically withdrawn from your account on a regular basis.Automatic payments: If you have set up automatic payments for bills or other expenses, the amount of the payment will be withdrawn from your account when it is due.Check payments: If you write a check to pay for something, the recipient may deposit the check and withdraw the funds from your account.Electronic transfers: You may also be subject to a cash withdrawal if you authorize an electronic transfer of funds from your account to another account.In summary, there are many ways that you could be subject to a cash withdrawal even if you do not use an ATM. It is important to carefully track your account balance and be aware of any automatic transactions or payments that may be taking place.\"\n",
    "result = predict(user_sentence)\n",
    "print(\"Predicted Class:\", result[\"class\"])\n",
    "print(\"Probabilities:\", result[\"probabilities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
